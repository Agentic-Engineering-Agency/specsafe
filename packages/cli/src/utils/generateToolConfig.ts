import { writeFile, mkdir } from 'fs/promises';
import { existsSync } from 'fs';
import chalk from 'chalk';

/**
 * Generates configuration files for AI coding tools
 */

const cursorRulesContent = `# SpecSafe Rules for Cursor

You are working on a SpecSafe project. Follow these rules:

## Always Check PROJECT_STATE.md
Before making changes, read PROJECT_STATE.md to understand:
- Current active specs and their stages
- Which spec is being worked on
- Requirements that must be satisfied

## Spec-Driven Workflow
1. Specs define WHAT to build (requirements, scenarios)
2. Tests define HOW to verify (generated by SpecSafe)
3. Implementation satisfies tests

## Stage-Aware Development
- SPEC stage: Generate tests from requirements
- TEST stage: Tests exist, implement code to pass them
- CODE stage: Implementation in progress, maintain tests
- QA stage: Validate against scenarios
- COMPLETE: Feature done, archive spec

## Never
- Skip tests to implement faster
- Modify specs without updating PROJECT_STATE.md
- Commit code without corresponding spec entry

## Always
- Run tests before marking work complete
- Update spec stage using \`specsafe\` commands
- Reference spec ID in commit messages
`;

const continueConfigContent = {
  customCommands: [
    {
      name: 'specsafe',
      description: 'Show current SpecSafe status',
      prompt: 'Read PROJECT_STATE.md and show me the current active specs, their stages, and what needs to be done next.',
    },
    {
      name: 'spec',
      description: 'Show details for a specific spec',
      prompt: 'Read the spec file for {{input}} from specs/active/ and show me its requirements, scenarios, and current stage.',
    },
  ],
  contextProviders: [
    {
      name: 'specsafe-state',
      params: {
        file: 'PROJECT_STATE.md',
      },
    },
  ],
};

const aiderConfigContent = `# Aider configuration for SpecSafe projects

# Always read PROJECT_STATE.md for context
read:
  - PROJECT_STATE.md

# Instructions for the AI
assistant_prompt: |
  You are working on a SpecSafe project. Always:
  1. Check PROJECT_STATE.md for current specs and stages
  2. Ensure implementation satisfies test requirements
  3. Update spec stage using specsafe commands when complete
  4. Never skip tests or modify specs without tracking

# Files to ignore
ignore:
  - specs/archive/
  - node_modules/
  - dist/
  - .git/
`;

const zedSettingsContent = {
  assistant: {
    default_model: {
      provider: 'anthropic',
      model: 'claude-3-5-sonnet-latest',
    },
    version: '2',
  },
  context_servers: {
    specsafe: {
      command: 'cat',
      args: ['PROJECT_STATE.md'],
    },
  },
};

/**
 * Generate configuration for a specific tool
 * @param tool - The tool name (cursor, continue, aider, zed, claude-code, crush)
 * @param projectDir - The project directory path
 */
export async function generateToolConfig(tool: string, projectDir: string = '.'): Promise<void> {
  switch (tool) {
    case 'cursor':
      await generateCursorConfig(projectDir);
      break;
    case 'continue':
      await generateContinueConfig(projectDir);
      break;
    case 'aider':
      await generateAiderConfig(projectDir);
      break;
    case 'zed':
      await generateZedConfig(projectDir);
      break;
    case 'claude-code':
      await generateClaudeCodeConfig(projectDir);
      break;
    case 'crush':
      await generateCrushConfig(projectDir);
      break;
    default:
      throw new Error(`Unknown tool: ${tool}`);
  }
}

async function generateCursorConfig(projectDir: string): Promise<void> {
  const configPath = `${projectDir}/.cursorrules`;
  
  if (existsSync(configPath)) {
    console.log(chalk.yellow('‚ö† .cursorrules already exists, skipping'));
    return;
  }
  
  await writeFile(configPath, cursorRulesContent);
}

async function generateContinueConfig(projectDir: string): Promise<void> {
  const configDir = `${projectDir}/.continue`;
  const configPath = `${configDir}/config.json`;
  
  if (!existsSync(configDir)) {
    await mkdir(configDir, { recursive: true });
  }
  
  if (existsSync(configPath)) {
    console.log(chalk.yellow('‚ö† .continue/config.json already exists, skipping'));
    return;
  }
  
  await writeFile(configPath, JSON.stringify(continueConfigContent, null, 2));
}

async function generateAiderConfig(projectDir: string): Promise<void> {
  const configPath = `${projectDir}/.aider.conf.yml`;
  
  if (existsSync(configPath)) {
    console.log(chalk.yellow('‚ö† .aider.conf.yml already exists, skipping'));
    return;
  }
  
  await writeFile(configPath, aiderConfigContent);
}

async function generateZedConfig(projectDir: string): Promise<void> {
  const configDir = `${projectDir}/.zed`;
  const configPath = `${configDir}/settings.json`;
  
  if (!existsSync(configDir)) {
    await mkdir(configDir, { recursive: true });
  }
  
  if (existsSync(configPath)) {
    console.log(chalk.yellow('‚ö† .zed/settings.json already exists, skipping'));
    return;
  }
  
  await writeFile(configPath, JSON.stringify(zedSettingsContent, null, 2));
}

// ============================================================================
// OpenSpec-style Claude Code Skills for SpecSafe v0.4.0
// ============================================================================

const claudeSkillSpecsafeContent = `---
name: specsafe
description: Show SpecSafe project status and workflow guidance
disable-model-invocation: true
---

You are in a SpecSafe project using spec-driven development.

Read PROJECT_STATE.md and provide:
1. Summary of active specs and their current stages
2. Which specs need attention
3. Recommended next actions based on the project state
4. Brief reminder of the SDD workflow (SPEC ‚Üí TEST ‚Üí CODE ‚Üí QA ‚Üí COMPLETE)
`;

// 1. specsafe-new - Create new spec with PRD, tech stack, rules
const claudeSkillNewContent = `---
name: specsafe-new
description: Create a new spec with PRD, tech stack, and rules. Initializes SPEC stage.
disable-model-invocation: true
---

Create a new SpecSafe specification (NEW ‚Üí SPEC stage).

**When to use:**
- Starting a new feature
- Need to define requirements before coding
- Want to establish tech stack and rules upfront

**Input**: Feature name or brief description

**Steps**

1. **Validate project state**

   Check PROJECT_STATE.md exists (run \`specsafe init\` if not).
   Review current active specs to avoid conflicts.

2. **Create the spec**

   \`\`\`bash
   specsafe new "<feature-name>"
   \`\`\`

   This:
   - Generates a new spec file in \`specs/active/\`
   - Assigns a unique SPEC-ID (e.g., SPEC-20260211-001)
   - Sets status to SPEC stage
   - Updates PROJECT_STATE.md

3. **Initialize PRD structure**

   Open the new spec file and populate:
   - **Purpose (WHY)**: One paragraph explaining why this feature exists
   - **Scope (WHAT)**: In scope and out of scope sections
   - **Tech Stack**: Target frameworks, languages, tools
   - **Rules**: Coding standards, constraints, conventions

4. **Define initial requirements**

   Add to Requirements table:
   - Functional requirements (what the system must do)
   - Non-functional requirements (performance, security)
   - Priority levels (P0=must have, P1=should have, P2=nice to have)

5. **Show next steps**

   Display:
   - Spec file location
   - Current stage (SPEC)
   - Next command: \`/specsafe:spec <id>\` to flesh out details

**Output**

After creation:
- ‚úÖ Spec file created at \`specs/active/SPEC-YYYYMMDD-NNN-<feature-name>.md\`
- ‚úÖ Status set to SPEC stage
- ‚úÖ PROJECT_STATE.md updated
- üìã Prompt: "Edit the spec to add requirements, then run \`/specsafe:spec <id>\`"

**Guardrails**
- Feature name should be descriptive (kebab-case recommended)
- Check for naming conflicts with existing specs
- Tech stack decisions should reference existing project standards
- Never start coding without a spec in SPEC or later stage
`;

// 2. specsafe-spec - Generate detailed spec from PRD
const claudeSkillSpecContent = `---
name: specsafe-spec
description: Generate detailed specification from PRD. Fleshes out requirements and scenarios.
disable-model-invocation: true
---

Flesh out a detailed specification from the initial PRD (SPEC stage refinement).

**When to use:**
- PRD is drafted but needs detailed requirements
- Need to define acceptance criteria and scenarios
- Preparing to move to TEST stage

**Input**: The spec ID (e.g., SPEC-20260211-001)

**Steps**

1. **Read the spec file**

   Load \`specs/active/<spec-id>.md\` and review:
   - Current Purpose and Scope sections
   - Existing requirements (if any)
   - Tech stack decisions
   - Current stage (must be SPEC)

2. **Expand requirements**

   For each requirement in the PRD:
   - Assign unique requirement ID (e.g., FR-1, NFR-1)
   - Define clear acceptance criteria in Given-When-Then format
   - Specify priority (P0/P1/P2)
   - Add any dependencies or constraints

3. **Define scenarios**

   Create detailed scenarios covering:
   - **Happy path**: Normal operation
   - **Edge cases**: Boundary conditions
   - **Error cases**: Invalid inputs, failures
   - **Integration**: Interaction with other systems

   Format each scenario:
   \`\`\`
   ## Scenario: [Name]
   **Given** [precondition]
   **When** [action]
   **Then** [expected result]
   \`\`\`

4. **Technical approach**

   Expand the Technical Approach section:
   - Architecture decisions
   - API contracts (if applicable)
   - Data models
   - Integration points

5. **Test strategy**

   Define testing approach:
   - Unit test coverage targets
   - Integration test requirements
   - E2E scenarios to cover
   - Mock/stub requirements

6. **Implementation plan**

   Create phased implementation plan:
   | Phase | Task | Est. | Dependencies |
   |-------|------|------|--------------|
   | 1 | Setup | 30m | None |
   | 2 | Core logic | 2h | Phase 1 |

7. **Update status**

   Mark spec as ready for TEST stage if complete:
   \`\`\`bash
   specsafe spec "<spec-id>"
   \`\`\`

8. **Show summary**

   Display:
   - Requirements count by priority
   - Scenarios defined
   - Estimated total effort
   - Next command: \`/specsafe:test <id>\`

**Output**

After specification:
- ‚úÖ Detailed requirements with acceptance criteria
- ‚úÖ Comprehensive scenarios (happy path + edge cases)
- ‚úÖ Technical approach documented
- ‚úÖ Test strategy defined
- ‚úÖ Implementation plan with estimates
- üìã Status: SPEC (ready for TEST stage)
- üìã Prompt: "Ready to generate tests? Run \`/specsafe:test <id>\`"

**Guardrails**
- Requirements must have clear acceptance criteria
- Scenarios should cover at least: happy path, 2 edge cases, 1 error case
- Estimates should be realistic (use T-shirt sizes or hours)
- Do NOT proceed to TEST stage until requirements are complete
`;

// 3. specsafe-test-create - Create tests from scenarios
const claudeSkillTestCreateContent = `---
name: specsafe-test-create
description: Create tests from scenarios. Moves spec from SPEC to TEST stage.
disable-model-invocation: true
---

Generate tests from spec scenarios (SPEC ‚Üí TEST stage).

**When to use:**
- Spec requirements are complete
- Ready to generate test code
- Starting TDD cycle

**Input**: The spec ID (e.g., SPEC-20260211-001)

**Steps**

1. **Validate spec is ready**

   Check \`specs/active/<spec-id>.md\`:
   - Status must be SPEC stage
   - Requirements have acceptance criteria
   - Scenarios are defined
   - If incomplete, prompt to complete with \`/specsafe:spec\`

2. **Analyze tech stack**

   Read spec for testing framework:
   - Unit test framework (Vitest/Jest/Mocha)
   - E2E framework (Playwright/Cypress)
   - Mocking approach
   - Coverage requirements

3. **Generate test structure**

   Create test file(s) in appropriate location:
   \`\`\`
   tests/
   ‚îú‚îÄ‚îÄ unit/<feature-name>.test.ts       # Unit tests
   ‚îú‚îÄ‚îÄ integration/<feature-name>.test.ts # Integration tests
   ‚îî‚îÄ‚îÄ e2e/<feature-name>.spec.ts        # E2E tests
   \`\`\`

4. **Convert scenarios to tests**

   For each scenario in the spec:
   - Create test case with descriptive name
   - Write failing test (RED phase of TDD)
   - Include Given-When-Then in test comments
   - Reference requirement ID in test

5. **Create test utilities**

   Generate helpers if needed:
   - Mock data factories
   - Test fixtures
   - Setup/teardown helpers
   - Custom matchers

6. **Move to TEST stage**

   \`\`\`bash
   specsafe test "<spec-id>"
   \`\`\`

   This:
   - Updates spec status to TEST
   - Records test file locations
   - Updates PROJECT_STATE.md

7. **Show test summary**

   Display:
   - Number of tests created
   - Test file locations
   - Coverage target
   - Next command: \`/specsafe:dev <id>\`

**Output**

After test generation:
- ‚úÖ Test files created (all tests failing initially)
- ‚úÖ Scenarios converted to test cases
- ‚úÖ Test utilities/helpers created
- ‚úÖ Status: TEST stage
- üìã Prompt: "Tests are ready. Run \`/specsafe:dev <id>\` to implement"

**Guardrails**
- All tests must initially FAIL (TDD principle)
- Each test maps to a specific requirement
- Tests should be independent (no shared state)
- Include both positive and negative test cases
- Mock external dependencies appropriately
- Do NOT write implementation code at this stage
`;

// 4. specsafe-test-apply - Apply and run tests against implementation
const claudeSkillTestApplyContent = `---
name: specsafe-test-apply
description: Run tests against implementation and verify they pass. Loops back on failure.
disable-model-invocation: true
---

Apply and run tests against implementation (CODE ‚Üí QA stage).

**When to use:**
- Implementation is complete
- Ready to verify against tests
- Running the test suite to check pass/fail

**Input**: The spec ID (e.g., SPEC-20260211-001)

**Steps**

1. **Validate implementation state**

   Check \`specs/active/<spec-id>.md\`:
   - Status should be CODE stage
   - Implementation files exist
   - Tests exist and are ready to run

2. **Run the test suite**

   Execute tests for this spec:
   ```bash
   pnpm test <spec-id>  # or npm test -- <spec-id>
   ```

3. **Analyze results**

   **If ALL tests PASS:**
   - ‚úÖ Implementation is complete
   - Suggest: \`/specsafe:done <id>\` to mark complete
   
   **If ANY tests FAIL:**
   - ‚ùå Show specific failure messages
   - Identify which requirements are not met
   - Suggest fixes or return to implementation
   - **CRITICAL: Do not proceed to done until all tests pass**

4. **Loop back on failure**

   If tests fail:
   - Display failure details
   - Identify gaps between spec and implementation
   - Suggest \`/specsafe:test-apply <id>\` to retry after fixes
   - Update PROJECT_STATE.md with findings

**Output**

After test run:
- ‚úÖ All pass ‚Üí Ready for \`/specsafe:done\`
- ‚ùå Some fail ‚Üí Back to implementation
- üìä Test results summary with pass/fail counts

**Guardrails**
- ALL tests must pass before marking done
- Never ignore failing tests
- Each failure maps to a specific requirement gap
- Loop back to implementation until tests pass
- This is the quality gate - do not bypass it`;

// 5. specsafe-verify - Run tests, loop back on failure (alias/shortcut)
   \`\`\`

2. **Read requirements**

   Review spec for implementation guidance:
   - Requirements with acceptance criteria
   - Technical approach section
   - Architecture decisions
   - Integration points

3. **Implement to pass tests**

   Follow TDD cycle:
   
   **RED**: Confirm tests fail
   \`\`\`bash
   pnpm test
   # Should see failing tests
   \`\`\`
   
   **GREEN**: Write minimal code to pass tests
   - Start with simplest implementation
   - Don't over-engineer
   - Focus on making tests pass
   
   **REFACTOR**: Clean up while keeping tests green
   - Improve code quality
   - Extract functions/classes
   - Add documentation
   - Run tests after each change

4. **Track progress**

   For each test file:
   - Run specific test: \`pnpm test <pattern>\`
   - Fix failing tests one at a time
   - Verify no regressions

5. **Handle edge cases**

   As tests pass, review:
   - Edge case coverage
   - Error handling
   - Input validation
   - Performance considerations

6. **Move to CODE stage**

   When all tests pass:
   \`\`\`bash
   specsafe code "<spec-id>"
   \`\`\`

   This:
   - Updates spec status to CODE
   - Records implementation files
   - Updates PROJECT_STATE.md

7. **Show implementation summary**

   Display:
   - Files created/modified
   - Test results (all passing)
   - Coverage metrics
   - Next command: \`/specsafe:verify <id>\`

**Output**

After development:
- ‚úÖ Implementation code written
- ‚úÖ All tests passing
- ‚úÖ Code follows project conventions
- ‚úÖ Status: CODE stage
- üìã Prompt: "Implementation complete. Run \`/specsafe:verify <id>\` to validate"

**Guardrails**
- NEVER skip tests to implement faster
- Write only enough code to pass tests
- Keep tests passing throughout refactoring
- Follow existing code style and patterns
- Commit frequently with meaningful messages
- Reference spec ID in commits: \`feat(SPEC-001): add user auth\`
- If tests need changes, discuss with user first

**TDD Reminders**
- üîÑ RED: Write failing test first
- üîÑ GREEN: Write code to pass
- üîÑ REFACTOR: Clean up with tests green
- üö´ Never write code without a failing test
- üö´ Never skip the refactor phase
`;

// 5. specsafe-verify - Run tests, loop back on fail
const claudeSkillVerifyContent = `---
name: specsafe-verify
description: Run tests and validate implementation. Loops back to dev if tests fail. Moves spec from CODE to QA stage.
disable-model-invocation: true
---

Run tests and validate implementation (CODE ‚Üí QA stage). Loops back to dev if tests fail.

**When to use:**
- Implementation appears complete
- Need to validate against spec requirements
- Preparing for QA review
- Before marking spec as complete

**Input**: The spec ID (e.g., SPEC-20260211-001)

**Steps**

1. **Validate CODE stage**

   Check \`specs/active/<spec-id>.md\`:
   - Status must be CODE stage
   - Implementation files exist
   - Previous test run was passing

2. **Run full test suite**

   Execute all tests with coverage:
   \`\`\`bash
   pnpm test --coverage  # or equivalent
   \`\`\`

3. **Analyze results**

   Check for:
   - ‚ùå **FAILING TESTS**: Loop back to dev
   - ‚ö†Ô∏è **LOW COVERAGE**: Flag for improvement
   - ‚úÖ **ALL PASSING**: Proceed to validation

4. **If tests FAIL ‚Üí Loop to dev**

   Actions:
   - Show failing test names
   - Analyze failure patterns
   - Suggest fixes
   - Prompt: \`/specsafe:dev <id>\` to fix

   **Do NOT proceed to QA with failing tests**

5. **If tests PASS ‚Üí Validate against spec**

   Cross-reference implementation with requirements:
   - ‚úÖ All P0 requirements satisfied?
   - ‚úÖ All scenarios covered?
   - ‚úÖ Edge cases handled?
   - ‚úÖ No unintended side effects?

6. **Generate QA report**

   Document validation results with test counts, coverage, and GO/NO-GO recommendation.

7. **Move to QA stage**

   If validation passes:
   \`\`\`bash
   specsafe qa "<spec-id>"
   \`\`\`

   This:
   - Updates spec status to QA
   - Archives QA report
   - Updates PROJECT_STATE.md

8. **Show verification summary**

   Display:
   - Test results (pass/fail counts)
   - Coverage percentage
   - Requirements satisfaction
   - QA recommendation
   - Next: \`/specsafe:done <id>\` or back to \`/specsafe:dev <id>\`

**Output**

**If tests FAIL:**
- ‚ùå Test failure report
- üìã Analysis of failures
- üîß Suggested fixes
- üìã Prompt: "Fix issues and run \`/specsafe:dev <id>\` again"

**If tests PASS:**
- ‚úÖ All tests passing
- ‚úÖ Coverage report
- ‚úÖ QA validation complete
- ‚úÖ Status: QA stage
- üìã Prompt: "Ready to complete? Run \`/specsafe:done <id>\`"

**Guardrails**
- ‚õî NEVER proceed to QA with failing tests
- ‚õî NEVER override test failures
- Coverage target: minimum 80% (prefer 90%+)
- All P0 requirements must be satisfied
- QA report must be generated for traceability
`;

// 6. specsafe-done - Mark complete, archive spec
const claudeSkillDoneContent = `---
name: specsafe-done
description: Mark spec complete and archive. Moves spec from QA to COMPLETE stage.
disable-model-invocation: true
---

Mark a specification as complete and archive it (QA ‚Üí COMPLETE stage).

**When to use:**
- QA validation passed with GO recommendation
- All requirements satisfied
- Ready to mark as production-ready
- Feature is done and tested

**Input**: The spec ID (e.g., SPEC-20260211-001)

**Steps**

1. **Validate QA passed**

   Check \`specs/active/<spec-id>.md\`:
   - Status must be QA stage
   - QA report exists and recommends GO
   - All critical issues resolved

   Verify QA report shows:
   - All tests passing
   - Requirements satisfied
   - Coverage targets met

2. **Final review**

   Confirm with user (if interactive):
   - "All P0 requirements satisfied?"
   - "Tests passing with adequate coverage?"
   - "Ready to mark as complete?"

3. **Complete the spec**

   \`\`\`bash
   specsafe complete "<spec-id>"
   \`\`\`

   This:
   - Moves spec status to COMPLETE
   - Moves spec file to \`specs/completed/\`
   - Archives QA report
   - Updates PROJECT_STATE.md
   - Records completion timestamp

4. **Generate completion summary**

   Document the completed work with metrics, artifacts, and notes.

5. **Update PROJECT_STATE.md**

   Record:
   - Completed specs list
   - Total completed count
   - Recent completions

6. **Show completion confirmation**

   Display:
   - ‚úÖ Completion confirmation
   - Spec moved to \`specs/completed/\`
   - Metrics summary
   - Project stats update
   - Next steps prompt

**Output**

After completion:
- ‚úÖ Spec marked COMPLETE
- ‚úÖ Spec archived to \`specs/completed/\`
- ‚úÖ QA report archived
- ‚úÖ PROJECT_STATE.md updated
- ‚úÖ Completion report generated
- üìã Prompt: "Ready to start a new spec? Run \`/specsafe:new <feature>\`"

**Guardrails**
- ‚õî Spec must be in QA stage
- ‚õî QA report must recommend GO (not NO-GO)
- ‚õî Do NOT complete if critical issues remain
- ‚úÖ Confirm all P0 requirements met
- ‚úÖ Verify test coverage meets targets
- Archive all related artifacts
- Keep completed specs for historical reference
`;

// 7. specsafe-explore - Pre-spec exploration
const claudeSkillExploreContent = `---
name: specsafe-explore
description: Pre-spec exploration. Research and spike before creating formal spec.
disable-model-invocation: true
---

Pre-specification exploration mode - research and spike before creating formal spec.

**When to use:**
- Unclear requirements or scope
- Need to research technical approach
- Want to prototype before committing
- Evaluating feasibility of a feature
- Comparing alternative solutions

**Input**: Feature idea or problem statement

**Steps**

1. **Understand the problem**

   Clarify with user:
   - What problem are we solving?
   - Who are the stakeholders?
   - What are the constraints?
   - What's the expected timeline?

2. **Research existing solutions**

   Explore:
   - Industry best practices
   - Similar features in other products
   - Open source libraries available
   - Design patterns applicable

3. **Spike/prototype (optional)**

   If technical feasibility is uncertain:
   - Create minimal proof-of-concept
   - Test critical assumptions
   - Measure performance characteristics
   - Document findings

   Keep spike code separate:
   \`\`\`
   spikes/
   ‚îî‚îÄ‚îÄ <feature-name>-exploration/
       ‚îú‚îÄ‚îÄ README.md
       ‚îî‚îÄ‚îÄ [prototype files]
   \`\`\`

4. **Document findings**

   Create exploration notes covering problem statement, research findings, options with pros/cons, recommendation, open questions, and next steps.

5. **Evaluate feasibility**

   Assess:
   - Technical complexity
   - Resource requirements
   - Timeline feasibility
   - Risk factors
   - Dependencies

6. **Recommend path forward**

   Decide:
   - ‚úÖ **Create spec**: Ready to formalize
   - üîÑ **More exploration**: Need additional research
   - ‚ùå **Not viable**: Too complex or not aligned
   - ‚úÇÔ∏è **Split**: Break into multiple smaller specs

7. **Transition to formal spec (if ready)**

   If exploration yields clear requirements:
   \`\`\`
   Exploration complete ‚Üí /specsafe:new <feature-name>
   \`\`\`

**Output**

After exploration:
- üìã Exploration notes documented
- üîç Research findings recorded
- üí° Options compared with pros/cons
- üéØ Feasibility assessment
- üìä Effort estimates
- üö¶ Recommendation (proceed/pivot/abandon)

**If proceeding:**
- üìã Prompt: "Ready to create spec? Run \`/specsafe:new <feature-name>\`"

**Guardrails**
- Exploration is time-boxed (suggest 2-4 hours max)
- Document everything for future reference
- Don't over-engineer prototypes (throwaway code OK)
- Be honest about risks and unknowns
- Involve stakeholders before committing to spec
- Exploration ‚â† Implementation (no production code)
`;

async function generateClaudeCodeConfig(projectDir: string): Promise<void> {
  // Create CLAUDE.md project context file
  const configPath = `${projectDir}/CLAUDE.md`;
  
  if (!existsSync(configPath)) {
    const claudeContent = `# SpecSafe Project ‚Äî Claude Code Configuration

You are working on a SpecSafe project using spec-driven development (SDD).

## Project Context

**PROJECT_STATE.md** ‚Äî Always read this file first. It contains:
- Active specs and their current stages
- Which spec is being worked on
- Overall project status

**Specs directory** ‚Äî \\\`specs/active/*.md\\\` contains detailed spec files with:
- Requirements (must be satisfied)
- Scenarios (acceptance criteria)
- Current stage (SPEC ‚Üí TEST ‚Üí CODE ‚Üí QA ‚Üí COMPLETE)

## Spec-Driven Development Workflow

1. **SPEC stage**: Spec defines WHAT to build (requirements, scenarios)
2. **TEST stage**: Tests define HOW to verify (generated by SpecSafe)
3. **CODE stage**: Implementation satisfies the tests
4. **QA stage**: Validate against scenarios, edge cases
5. **COMPLETE**: Feature done, spec archived

## Critical Rules

‚úÖ **ALWAYS** read PROJECT_STATE.md before making changes  
‚úÖ **ALWAYS** ensure implementation satisfies tests  
‚úÖ **ALWAYS** use \\\`specsafe\\\` CLI commands to advance stages  
‚úÖ **ALWAYS** reference spec ID in commit messages  

‚ùå **NEVER** modify PROJECT_STATE.md directly (use CLI)  
‚ùå **NEVER** skip tests to implement faster  
‚ùå **NEVER** modify specs without updating state

## SpecSafe CLI Reference

- \\\`specsafe status\\\` ‚Äî Show current project status
- \\\`specsafe new <name>\\\` ‚Äî Create new spec (NEW ‚Üí SPEC)
- \\\`specsafe spec <id>\\\` ‚Äî View/edit spec details
- \\\`specsafe test <id>\\\` ‚Äî Generate tests (SPEC ‚Üí TEST)
- \\\`specsafe code <id>\\\` ‚Äî Start implementation (TEST ‚Üí CODE)
- \\\`specsafe qa <id>\\\` ‚Äî Run QA validation (CODE ‚Üí QA)
- \\\`specsafe complete <id>\\\` ‚Äî Complete spec (QA ‚Üí COMPLETE)

## Claude Code Skills (OpenSpec-style)

This project includes Claude Code skills for slash commands:

| Command | Stage | Purpose |
|---------|-------|---------|
| \\\`/specsafe:explore\\\` | PRE | Research before formal spec |
| \\\`/specsafe:new\\\` | NEW ‚Üí SPEC | Create spec with PRD |
| \\\`/specsafe:spec\\\` | SPEC | Flesh out detailed spec |
| \\\`/specsafe:test\\\` | SPEC ‚Üí TEST | Generate tests from scenarios |
| \\\`/specsafe:dev\\\` | TEST ‚Üí CODE | Implement to pass tests |
| \\\`/specsafe:verify\\\` | CODE ‚Üí QA | Run tests, loop if fail |
| \\\`/specsafe:done\\\` | QA ‚Üí COMPLETE | Mark complete, archive |
| \\\`/specsafe\\\` | ANY | Show project status |

## Workflow Example

\`\`\`bash
# 1. Explore an idea
/specsafe:explore "user authentication"

# 2. Create formal spec
/specsafe:new "user-authentication"

# 3. Flesh out details
/specsafe:spec SPEC-20260211-001

# 4. Generate tests (TDD - all fail)
/specsafe:test SPEC-20260211-001

# 5. Implement to pass (RED ‚Üí GREEN ‚Üí REFACTOR)
/specsafe:dev SPEC-20260211-001

# 6. Verify (loops back if tests fail)
/specsafe:verify SPEC-20260211-001

# 7. Complete when QA passes
/specsafe:done SPEC-20260211-001
\`\`\`
`;
    
    await writeFile(configPath, claudeContent);
    console.log(chalk.green('‚úì Created CLAUDE.md'));
  } else {
    console.log(chalk.yellow('‚ö† CLAUDE.md already exists, skipping'));
  }

  // Create .claude/skills/ directory with SKILL.md files for slash commands
  const skillsDir = `${projectDir}/.claude/skills`;
  
  if (!existsSync(skillsDir)) {
    await mkdir(skillsDir, { recursive: true });
  }

  // Helper function to create a skill
  async function createSkill(skillDirName: string, content: string): Promise<void> {
    const skillDir = `${skillsDir}/${skillDirName}`;
    const skillPath = `${skillDir}/SKILL.md`;
    
    if (!existsSync(skillPath)) {
      if (!existsSync(skillDir)) {
        await mkdir(skillDir, { recursive: true });
      }
      await writeFile(skillPath, content);
      console.log(chalk.green(`‚úì Created .claude/skills/${skillDirName}/SKILL.md`));
    } else {
      console.log(chalk.yellow(`‚ö† .claude/skills/${skillDirName}/SKILL.md already exists, skipping`));
    }
  }

  // Create all OpenSpec-style skills
  await createSkill('specsafe', claudeSkillSpecsafeContent);
  await createSkill('specsafe-new', claudeSkillNewContent);
  await createSkill('specsafe-spec', claudeSkillSpecContent);
  await createSkill('specsafe-test-create', claudeSkillTestCreateContent);
  await createSkill('specsafe-test-apply', claudeSkillTestApplyContent);
  await createSkill('specsafe-verify', claudeSkillVerifyContent);
  await createSkill('specsafe-done', claudeSkillDoneContent);
  await createSkill('specsafe-explore', claudeSkillExploreContent);
}

async function generateCrushConfig(projectDir: string): Promise<void> {
  // OpenCode/Crush uses .opencode/commands/ for custom commands
  const commandsDir = `${projectDir}/.opencode/commands`;
  
  if (!existsSync(commandsDir)) {
    await mkdir(commandsDir, { recursive: true });
  }

  // Create specsafe command
  const specsafeCmdPath = `${commandsDir}/specsafe.md`;
  if (!existsSync(specsafeCmdPath)) {
    const specsafeContent = `Show SpecSafe project status and workflow guidance

Read PROJECT_STATE.md and provide:
1. Summary of active specs and their current stages
2. Which specs need attention
3. Recommended next actions based on the project state
4. Brief reminder of the SDD workflow (SPEC ‚Üí TEST ‚Üí CODE ‚Üí QA ‚Üí COMPLETE)
`;
    await writeFile(specsafeCmdPath, specsafeContent);
    console.log(chalk.green('‚úì Created .opencode/commands/specsafe.md'));
  } else {
    console.log(chalk.yellow('‚ö† .opencode/commands/specsafe.md already exists, skipping'));
  }

  // Create spec command
  const specCmdPath = `${commandsDir}/spec.md`;
  if (!existsSync(specCmdPath)) {
    const specContent = `Show details for a specific spec by ID

Read the spec file from specs/active/$SPEC_ID.md and show:
- Requirements
- Scenarios/acceptance criteria
- Current stage
- Implementation files referenced

If no SPEC_ID provided, list available specs.
`;
    await writeFile(specCmdPath, specContent);
    console.log(chalk.green('‚úì Created .opencode/commands/spec.md'));
  } else {
    console.log(chalk.yellow('‚ö† .opencode/commands/spec.md already exists, skipping'));
  }

  // Create validate command
  const validateCmdPath = `${commandsDir}/validate.md`;
  if (!existsSync(validateCmdPath)) {
    const validateContent = `Validate current implementation against active spec

Check if the current code changes satisfy the requirements in the active spec.
Point out any gaps or issues that need to be addressed before completing.
`;
    await writeFile(validateCmdPath, validateContent);
    console.log(chalk.green('‚úì Created .opencode/commands/validate.md'));
  } else {
    console.log(chalk.yellow('‚ö† .opencode/commands/validate.md already exists, skipping'));
  }
}

/**
 * Generate git hooks configuration
 * @param projectDir - The project directory path
 */
export async function generateGitHooks(projectDir: string): Promise<void> {
  const hooksDir = `${projectDir}/.githooks`;
  const preCommitPath = `${hooksDir}/pre-commit`;
  
  if (!existsSync(hooksDir)) {
    await mkdir(hooksDir, { recursive: true });
  }
  
  const preCommitContent = `#!/bin/bash
# SpecSafe pre-commit hook

echo "üîç Running SpecSafe pre-commit checks..."

# Validate PROJECT_STATE.md exists
if [ ! -f "PROJECT_STATE.md" ]; then
  echo "‚ùå PROJECT_STATE.md not found. Run 'specsafe init' first."
  exit 1
fi

# Run spec validation (if we add a validate command)
# specsafe validate --silent || exit 1

echo "‚úÖ Pre-commit checks passed"
`;
  
  if (existsSync(preCommitPath)) {
    console.log(chalk.yellow('‚ö† .githooks/pre-commit already exists, skipping'));
    return;
  }
  
  await writeFile(preCommitPath, preCommitContent);
  
  // Make the hook executable (this won't work on Windows without special handling)
  try {
    const { exec } = await import('child_process');
    exec(`chmod +x ${preCommitPath}`);
  } catch {
    // Ignore chmod errors on Windows
  }
}
